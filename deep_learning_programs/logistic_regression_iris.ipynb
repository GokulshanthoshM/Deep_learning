{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnbtJiZiKYVy4BF4ToHViK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1jIRKz2O-8s","executionInfo":{"status":"ok","timestamp":1694670667040,"user_tz":-330,"elapsed":7807,"user":{"displayName":"Gokul Shanthosh","userId":"03657416323634609159"}},"outputId":"40d38ff1-4c49-46be-8453-b089c6aaecd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[[17  0  0]\n"," [ 0 15  0]\n"," [ 0  0 13]]\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":2}],"source":["import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#\"Importing the dataset\n","\"\"\"\n","After importing the necessary libraries, next, we import or read the dataset.\n"," Click here to download the breast cancer dataset used in this implementation.\n"," The breast cancer dataset has the following features:\n"," Sample code number, Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion,\n"," Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitosis, Class.\n","\"\"\"\n","\n","\n","# divide the dataset into concepts and targets. Store the concepts into X and targets into y.\n","dataset = pd.read_csv('/content/drive/MyDrive/Iris.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values\n","\n","#Splitting the dataset into the Training set and Test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 2)\n","\n","#Feature Scaling\n","\"\"\"\n","Feature scaling is the process of converting the data into a given range. In this case, the standard scalar technique is used.\n","from sklearn.preprocessing import StandardScaler\n","\"\"\"\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\"\"\"\n","Training the Logistic Regression (LR) Classification model on the Training set\n","Once the dataset is scaled, next, the Logistic Regression (LR) classifier algorithm is used to create a model.\n","The hyperparameters such as random_state to 0 respectively.\n"," The remaining hyperparameters Logistic Regression (LR) are set to default values.\n"," \"\"\"\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state = 0)\n","classifier.fit(X_train, y_train)\n","#Logistic Regression (LR) classifier model\n","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l2',\n","                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n","                   warm_start=False)\n","#Display the results (confusion matrix and accuracy)\n","\"\"\"\n","Here evaluation metrics such as confusion matrix and accuracy are used to evaluate the performance of the model\n","built using a decision tree classifier.\n","\"\"\"\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","y_pred = classifier.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy_score(y_test, y_pred)\n"]}]}